Kubernetes was developed by google, it manages containers

Open source container orchastration tool 
Helps you manage them in different environments -> like physical machiners, cloud machines or vms

 Managing thousands of containers spread through different environments throgh scripts can be tedious

also the trend from monolith to microservices resulted into the use of thousands of containers

Hence the increased demand for the proper way of managing those hundreds of containers

Tools like kunernetrs guarantee the folloeing:
=> High availability no downtime
 -> scalability and high performance
=> disaster recovery || back up and restore

K8 components explained:
Node => Is a simple server or a virtial machine

Pod => is a smallest unit of k8s
	it is an abstracion of a container, what pod does is create these running environents or a layer o	on top of a container, and the reason is kubernetes wants to abstract away the container runtime and 	container technologies so that you can replace them if you want to and because you dont want to work d	directly with docker or container technology you use in kubernetes so that you only interact with kubernetes layer
	 Pod us meant to run 1 container inside it 

'
Kunernetes offers a virtual netwoek out of the box which which means each pod gets its own ip adress
the pod gets its own ip adress not the container and each pod can communicate with each other using and jip adress which is an internal ip adress and not a public one, so ,my container can communicate with the database using an ip adress 

Note: Pods in kubernetes are ephemeral meaning they can die very easly

and in case of anything going wrong the pod will die and the new one will be creates and a new adress will be created and assigned a new ip adress, This makes it inconvinient as if comminication was through ip asress if the pod dies then the one cretes will lose communication with the newly created pos as they have different ip adress

due to that another component of kubernetes called service is used


SERVICE and INGRESS:
Service is a permanent ip adress that can be attached to each pod so my-app will have its own service and pod will have its own service, the lifecycle of a pod and a service are not connected so even if the pod dies the ip adress will stay 

External service is a service that opens the services to the external sources

Insteas of the esternal request going directly to the intenal services they will fitstly go to ingress then the fowarded to the internal services

CONFIGMAP and SECRETS:
configmap is an external configuration to your application eg urls of the databases etc, also you can put the database username and passwords but this would be insecure

secrets: its just like config map but used to secure secret data
They are not stored in plain text but in base 64 encoded format

The built in security mechanisms are not enabled ny default

You can use this as an environmenal variable or a proprty value


VOLUNES:
Data stprage and storage:
How it accomplishes this is by attaching the actual phydical drive to your pod which means it can be on the same local machine which means the same pod is running ot it could be a remote storage outside of a kubernetes cluster, it could be cloud storage or on premise storage which is not a part of kubernetes structure	you just have a reference to it

 KUbernetes doesnt hanhle data persistence so you hace to handle it yourself 


DEPLOYMENT and STATEFUL SETS:
IF my pod dies then i will have downtime which is a bad thig as my site wont be accesible

A service has two functionalities:
=> provides permanent ip adress
=> is a load balancer -> will catch a request and foward it to the the service which is least busy

To create a new pod you woulnt create another pod but define a blueprint and specify the number of pods you want to run. That blueprint is called deployment.

And you wont be working with pods you will be working with deployments because there you can specify the number if pods you want ans you can scale up or down number of replica of pods you need.

Note: Pod is layer of abstraction on top of containers and deployments are a layer of abstraction on top of pods

Now if one replicas of your application dies the service will foward the request to another one so that the application will be accesible to the user 

You cant replicate database using deployments, reason being database has state meaning its data this means if we have the replicas of database they will have to access the same storage and you will need some mechanisms to know which services are writing to that storage and which are reading from what storage in order to avoid data inconsistencies

deployments are for stateless apps
statefulSet for stateful apps or databases => the reads and writes for the databases ara synchronized

Deploying stateful sets can be tedious and thats why its common to host the stateful applications outside the container ans the stateless ones inside as they easly scale withought having to worry about data losses

SUMMARY OF KUBERNETES COMPONENTS:
=> Pod - abstraction of containers
=> Service - communication between pods
=> Ingress - used to route traffic into the cluster
=> External configuration usind Configmaps and Secrets
=> Volumes - handles data persistence
=> Blueprints using deployments and statefulSets

These are te core or the basic concepts, just using these you can build pretty powerful kubernetes cludters

K8 architecture explained:
Master processes
Each container will have multiple pods running in them 
-> three processes must be installed on every node

Worker nodes are the nodes that actually do the work, hence the name

The first proccess that need to run on every node is a container runtime
the processes that schedules the pods and the containers underneath is KUBELET
which is process of kubernetes itself unlike container runtime
Kubeletes interacts with both the node and the container 

Kubeletes starts the pod with the container inside and assigning resources to the container like cou ram and storage resoutces 
Usually kubernetes cluster is made up of multiple nodes which must have container services and kuberkets installed 

The third service that is respnsible for fowarding requests from services to pods is kube proxy that must be installed on every node and kubeproxy have an intelligent fowarding inside that make sure the indoemation works in the perfotmant way with low overhead For example if my-app is make request to database the request is not fowaeded to a random database but the satabase pod inside the same pod and hence avoiding the netwoek overhead of sending the request to the other machine

SUMMARY: 3Processes
=>Kubelet
=>Kube proxy  -these two must be installed on every node
=> container runtime


INTERRACTION WITH THE CLUSTER:

How to schedule a pod?
monitor?
reschedule or restart a pod?
join a ner node?

All these processes are done by master nodes: 4 of these process are:


=> Api server -> is like a cluster gateway its a way you can interact with the woeker nodes using kubernetes dashboard or kubelet cli
it acts as a gate keeper  for authentication to make sure only the authenticated users gets to the cluster

if you have a request => API server => Validates request => other processes => pod 
and if you eant to query the cluster health or the status of your deployment you make the request throgh the API server and it gives you the response 

2ndly: Scheduler
Schedile a new pod: API server -> scheduler => pod(the secheduler is intelligent to decide on which node pod will be scheduled or bext component will be scheduled)

Firstly it will look at your request and see how muvh of the resources you need and its gonna go through the woeker nodes and see the availavle resources in each and one of them, on the least busy is when the pod will be scheduled

TIP: Scheduler decides on which node the new pod will be scheduled, the process that actually start that container is a cubelet 


#rprocess: Controller manager
It detects state changes like crashes, and tries to recover the cluster state as soon as possible 

so contoller manager=> scheduler => Kubelet


4thrly etcd
This is like a cluster brain 
Cluster changes are stored in the key value store, and the reason its a cluster brain is because all these mechanisms with controller etc woek works because of the etcd data
For example how does resources available on each worker node, did the cluster state change? is the cluster healthy? all these are answered by having the info from etcd cluster

what is not stored:
The actual application data
etcd just storest the cluster information used for master processes to communicate with the work processes and vice versa

These master processes are crucial and each master node runs its master processes where the API server is load balanced and etcd stores distributed storage across all the master nodes

The actual hardware of master and worker resources differ, master nodes are importance but the have less load od work so they need less resources where the worker nodes do the actual work to  run those containers and therefore they need more resources and as the application scales you can add more nodes to meet the required application resources, so if you need the extra master nodes you get the bare metal server install the master processes and add it to kubernetes cluster
 and the same applies if you need two woeker nodes you get two bare metal servers install the woeker node processes and all it to kubernetes cluster and this way you can infinitely increase the power of kubernetes cluster as it replication level and resource demand increases


MINIKUBE and KUBRCTL:
Production cluster setup
-> it will have multiple masters atleast two in production setup 
-> and multiple worker nodes

minikube is this one test/local cluster setup where it is a one node cluster  when master and worker processes both run on the same machine and this will have docker runtime pre-insalled

so this minikube will create a virtual box on your laptop
=> node runs in that virtual box 
=> 1 k8s cluster

which can be used for testing purposes 
now you have a cluster you need a way to interact with your cluster and thats where this comes in:

KUBECTL:
kubectl is a commandline tool for k8 structure

Note: Minicube runs both master and worker processes

You first have to interact with the api server to do any kubernetes work
 so to interact with Api server can be through api, ui or cli of which all of these are all clients

and kubectl is the most poweful of all the clients


IMPORTANT: kubectl isnt just for minikube clustet 	if yoou have a cloud cluster or hybrid cluster minikubectl is the one to interact with the setup

!!!!Since minikube uses virtualization hence virtualization has to be enabled in your machine
 and you need to install some tyoe jof some hypervisor 

COMMANDS:
Minikube start => Creates and start the cluster
 options --vm-driver=hyperkit

kubectl get nodes => gets status of nodes

minikube status => also gets you the status of the running kubes

kubectl version => gives the version of installed kubernetes -> this will show you the client and the server version of this 


IMPORTANT:
Minikube CLI is for starting or deleting the cluster

Kubectl CLU is for configuring the minikube cluster

BASIC KUBECTLCOMMANDS:
Prequesities:
Have minicube cli and kubectl installed in your system 

You are going to be using kubectl for the control of anything

commands: 
kubectl get nodes
kubectl get pods
kubectl get services

to create anything kubectl create -h will give you the man pages for creating

In creating there is no pod option, there is an abstraction and that is deployment and that will create pods underneath

to create the creation is made from image and this is the command:
kubectl create deployment nginx-depl image=nginx

-> this will download the latest image of nginx from docker Hub

To see the seployments you can do : kubectl get deployment and if you want to get the pod: kubectl get pod

when i create the deployment it is the blueprint for creating the pods and the most basic configuration is providing the name and the image 


Between deployment and a pod there is another another abstraction layer called replicaset which is autimatically managed by kuberneter deployments 

command to get the replicaser: kubectl get replicaset

the podname has the prefix of deployment and replicaset is and its own id

the replocaset is basically managing the replicas of a pod, You wont be manually required to create or delete a replicaset file but you gonna be working with deployments directly  to create multiple replicas you can provide te options to do that 


check this out:
a deployment manages a:
replicaset manages a:
a pod is an abstraction of 
a container 

 thats the hierrachy and everything below the deployment should be managed by kubernetes

to edit:
kubectl edit deployment [name] => this will give you the auto generated configuration file with default values because at the command if you gave name and an image everything else is auto generated

and if you edit it will open the text editor and yoou can change some configurations and after saving the info the old pod eg. if you changed the image version will be terminated and the new one will be start running 

51:37


kubectl get replicaset: => this will show the old and the new replicas of the same pod


DEBUGGING LOGS:
kubectl logs <logname> -> shows what the application inside the pod actually logged

describe gives you more information about the pod 
command: kubectl describe [pod name]

then run kubectl get pods

and to see the logs => kubectl logs [pod-name]

to get inside the terminal of the particular container or application
command: kubectl exec -it [podname] --/bin/bash

Deleting a deployment and Applying the configuration file:

To get the deployments: kubectl get deployment

IMPORTANT: deleting the deployment will delete the replicasets and the pods 
command: kubectl delete deployment [deployment name]

IMPORTANT: All the crude operations happens at the deployment level and everything underneath falls automatically

When you create the deployments you should provide the options like:
--name image option1 option2 and lots of other things you would want to configure and it would be impractical to write all that out on a commandline, so because of that in practice you would be working with the configuration files

The way to do it is to use the apply command which takes in the configuration file

command: kubectl apply -f [configuration-file.yaml]

You make the configuration file and then apply tha and ithat will take care of creting the deployment 

Summary:
create a deployment => kubectl create deployment [name]
Edit deployment => kubeclt edit deployment [name]
Delete deployment -> kubectl delete deployment [name]

status of different k8s components:
kubectl get nodes | pod | services | replicaset| deployment

Debugging pods:
log to a console => kubectl logs [pod name]
Get interactive terminal kubectl exec -it[podname] --bin/bash
Get info about pods=> kubectl describe pod [pod name]

Use configuration file to create deployments:
Apply a configuration file  => kubectl apply -f [file name]
Delete with a configuration file => kubectl delete -f [file name]

K8s yaml files:
composed of three main parts:
Specifications
Metadata
Version and type of component

the kind of specification will be speciefic to the kind of a component you are creating

The deployment will have its own attribute that only apply fot deployment and service will have its own stuff

The third part of the yaml file is status and will be automatically generated by kubernetes

Status:
K8s always compares what is the desired state and what is the actual state

and if the desired state do not match then k8s knows there is something to be fixed
and it is going to try to fix it and this is the basis of self healing that kubernetes tries to provide 

it will continously compare the status with the specifications and woll know there is an issue and will contonously try to fix it 

Where does k8s get the state to update continously: The info comes from etcd 
Etcd holds the current status of any k8s component and thats where the status information comes from 

Format of configuration file:
Yaml is very strict about the identation 

You can use an online yaml validator to see if there is any issue with your syntax

The usual practice is store these configuration files with your code, since deployment and service will be applied to your application code, usually it will be a part of iac infrastracture as code or you can have its own git repo for the configuration files

Layers of abstraction:
Deployments manages a:
Replicaset manages a:
Pod is an abstraction of:
Container

A pod shoul have its configurations inside the deployments too



Connecting components(Labels, selectors and ports):
Metadata contains the labels and specification part contains selectors

In metadata yoou give components value pairs and that label sticks to that component
Pod get the label through template blueprint
This label is matched by the selector 

To get the more information about the pod: kubectl get pod -o wide

kubectl get deployment nginx-deployment -o yaml and to save it > nginx-deployment-result.yaml it will be saved and you can make a comparison and try to see the added status  and other things added by kunernets by defailt


COMPELETE DEMO PROJRCT:
Overview of the components going to be involved:
2 deployment/pod
2 service (create internal service)
1 config map
1 secret

Request flow through the browser
Request comes from the browser,
=> goes through the external service of the mongo express 
=> which will foward it to the mongo express pod 
-> the pod will then connect with internal service of mongo db and is basically the url here  (configmap)
=> and will then fowart it to the mongodb data which will then authenticate the request using credentials\



Firstly have the minicube cluster running

kubectl get all gets you all the kubernetes componetns  that are in the cluster

and for now if you are using minikube then the only running default cube is the default minikube cluster

The first thing is creating a mongo db deployment

You have to refer to the dockerhub repository to get to see the required configurations or the optional configurations that are available

This will show you the default port and the environmental variables

When you define the yaml file its going to be defined in the repostory so its the bad practive to include the username and password there inside the configuration file there its not supposded to be plave there 

The secrets lives in k8s and not in the repository 
the git one 

Befire applying the deployment you first have to create the secrets where the credentials will live

SECREt CONFIG FILE:
Kind -> secret
-> metadata/name => a random name
type -> "opaque default for arbitary key value pairs


Other types include the tls certificate for tls certificate files 
the data with the following syntax will be used for the key value pairs:
data:
	mongo-root-username: the_name
	mongo-root-password: the_passkey

Storing data in secrets component doesnt make it autimatically secure 

There are built in meachanisms like encyrption for basic security  which are not enavled by defauly 

the simple way to do it is:
command: echo -n 'username' | base64

the output will be base 64
and the the same will be applid for the password

All that is preparation work

IMPORTANT: You have to create sectets before the deployment if you are going to reference the secrets in the file you are creating, so the order of creation matters


If you make the deployment of the secrets which doesnt exist yet you are going to get an error

You have to apply the secrets before even applying the deployment otherwise you are goung to get an error

after applying it to see the secrets you can run kubectl get secret  you will see if your secrets have been created

Then after you already have your references you can now reference in your deployments
in the deployments instead of values you have to say values from:
env: 
	name: some-env-variable
	valueFrom: 
		secretKeyRef:
			name:secret-name(The actual name in your file not the name of the file the name in the name section after ApiVersion)
So in the value you will output your key not the base64 actual value

Yaml is very strict with the indetation 

base64 just encodes the given string to remove the non ascii charaters but its not for encryption for actual encyrption you have to use tools like hascorp vault

But this encoding is still good since you dont want your code in your repository 

note: to apply the deployments:
command: kubectl apply -f mongo.yaml

to get everything:
command: kubectl get all 

To see whats going on in the pod you have to decribe the pod
command: kubectl describe pod <podname>

The second step is creating the external service so that the other pod can talk to this mongodb

You can create separate files for yaml files or with --- you can start a new document

Its okay to put deployment and a service in one file since they belong together

The way to connect the service to the pod is using selector  -=> to connect to the pod through the label, by using the label of the pods the service through the labels can find the pods to attach to 

port: service port
targerPort: container port for deployment

service port and targer port have tp match, They can be different but under what conditions??
ChatGPT said:

Great question! In Kubernetes, Service port and targetPort can be different, and this is actually a common and useful feature. Let me break down what each means and when you’d want them to match or differ.

What are port and targetPort in a Kubernetes Service?

port (or service port):
This is the port that the Service exposes to other pods or clients inside the cluster (or outside, if it's a NodePort or LoadBalancer service). It's the port clients use to connect to the service.

targetPort:
This is the port on the pod/container that the Service routes traffic to. It’s the actual port the application inside the pod listens on.

When do they need to match?

When your Service’s exposed port is the same as the container’s listening port.

Example:

spec:
  ports:
  - port: 80
    targetPort: 80


This means the Service listens on port 80, and forwards traffic to pods on port 80.

When and why can they be different?

If your application listens on a different port than what you want exposed externally.

Example:

spec:
  ports:
  - port: 80
    targetPort: 8080


Here, clients connect to the Service on port 80, but the Service forwards requests to port 8080 on the pod.

Why would you do this?

Abstraction:
Clients don’t need to know the actual port your app listens on inside pods. You can expose a standard port externally (like 80 or 443) regardless of the internal pod port.

Multiple ports:
Your pod might have multiple containers or multiple ports open, and the Service selectively exposes some.

Port remapping:
You may want to unify or change the exposed port without changing the app’s container port.

Additional notes

If targetPort is not specified, it defaults to the same value as port.

The targetPort can be specified either as a number or as a string referring to the port name defined in the Pod spec.

Example with named ports:

Pod Spec:

containers:
- name: app
  ports:
  - containerPort: 8080
    name: http


Service Spec:

ports:
- port: 80
  targetPort: http  # refers to containerPort named "http"

Summary
Condition	port	targetPort	Explanation
Simple, same port exposed	80	80	Service port matches container port
Expose different port externally	80	8080	Service port is 80; pod listens on 8080
Use named ports	80	"http"	targetPort refers to named containerPort
targetPort omitted	80	80 (default)	targetPort defaults to port value

If there are multiple k8d components defined in the same yaml file then the k9s will know which has not changed and what has changed when you apply them

If you edit both then both deployment and service will be changed

to see what port is connected to what port then you can run kubectl describe service and will list the target port and you can see the ports connection or mapping

to get extra information about the pos and to what service is connected to you can:
command: kubectl get pod -o wide

To get all the componets you can:
command: kubectl get all and you can grep them by name and get the exact pod you are looking for 

Next we are going to configure the mongo service and mongo express and create configmaps where the satabase utl is gonna be 


For the place to add the adress of the mongo database you can add it to the yaml file directly or you can use the configMap

configMap:
=> external configuration
=> centralized
=> Other components can use it

and then you can reference that external configuration in that file when the configuration is needed

configMap configuration file:
kind: configMap
metadata/name: a random name

data: which is the key value pairs
server name is the name of the service and its simple as that and that is going to be the database name url


Just like the secrets the order of execution matters, you have to apply the config files before you can apply them: IOW the configMap must be already in the cluster before you can reference it


The way you can reference the config maps inside the deployment is very similar to secrets and now instead of configMapRef its now gonna be configMapKeyRef and as it was for the secrets the mane is the actual name that was used in the name key from the configuration yaml file



External service to access Mongo from the browser

In practice you never have service without deployments so it makes sense to put them together

The external service file is declared the same way as the service even the kind of service, so how do you make this service external?
so below the selector im gonna put a type and the type of this is loadbalancer, (But internal service also is a load balancer so its kind of a bad name for sure)

What the load balancer type of service does is it assigns an external ip adress and so accepts external requests

also you will provide the node port:
This is the port where this external ip adress will be open, That will be the port for external ip adress and the port you wull put into your browser


AND THat must be between 30K to 32767

Internal service would not apply to any type

internal service(Cluster IP) is default; so you dont have to define it when creating the internal service, While the Loadbalancer type will have to be defined for an external service explicitly 

and in the external ip adress inside the minikube shows up as pending but for the real machine the public ip adredd will show up


minikube service service-name => This will assign external service an external ip adress

CHAIN OF COMMAND:
Browser request => Mongo express service => Mongo express pod -> Mongodb internal service => Mongodb pod 

KUBERNETES NAMESPACES EXPLAINED:
What is a namespace:
In k8s you can organize the cluster in namespaces, and you can have multiple namespaces in k8s cluster
=> Think of it like a virtual cluster inside the cluster

If you create a cluster the k8s gives you 4 namespaces by default
if you type: 
command: kubernetes get namespace you see the list of the namespaces
Explanation:
Kuberneters-dashboard only ships with minikube, it is speciefic to minikube installation, you wont have this in a standard cluster

kube-system =>> this namespace is not meant for your use, shouldnt name anything or modify anything in kube-system

the components which are deployed in kubesystem namespace are the system processes, master and kubectl processes

kube-public =>> contains the publically accesible data, It has a configMap that contains cluster information which is accesible even without authentication 
if you type kubectl cluster-info this will output the configurations


kube-node-lease =>> Which is the recent to kubernetes
Purpose:
Holds the heartbeat of the nodes, each node has its associated lease in namespace
determines the node availability

Fourth name is the default namespaace:
=> Will be used to create the resources that you are going to need, or the resource you create are going to be located there

You can create your own namespace 
command: kubectl create namespace <your_namespace>
to list them: kubectl get namespace

Another way is to use the namespace configuration file which is the better way to so that 

WHY USE NAMESPACES:
The best use of namespaces is to group the resources for instance you can have the database namespace and have inside of it all the required resources, Ypu can have monotoring namespaces where you deploy all the prometheous, and all the stuff it needs , You can have a namespace where the elastic search stack like kibana lives,  then you can have the other for nginx-ingress resources

This is just one way of grouping your resources inside of the cluster

According to the official k8s doc you shoulnt use namespaces if you have smaller projects, up to 10 users, But in general it is a good idea to group your namespaces

EVEN IN MINIMUM PROJECT USE NAMESPACES

Annother usecase scenario is when working in a team and all of them use the same deployment and would result into overriding the original file and can result into issues


another use case scenarion is if you want to host both the stating and development environment in the same cluster and the importance of this it will make it easier in the usage of environments which are common to both



another use case is when using the blue/green deployment, which means that in the same cluster and you want to have two versions of production, the one which is active now and the other one which is in the production next
The versions of the apps in the blue and green will differe but these might be in different namespacces but they will need to use the same resources, and in this way they can use the common shared resources without having to set up a separeate cluster


The other use case is to limit the acess and resources in the namespace, when working in teams, so that this way they are able to perform the crude operations in their own namespaces but cant do anything in any other namespace, this can minimize the risk if interfearences between teams. Each team has their own secure isolated environment

The other thing is you can limit the resources each namespace consume eg. CPU, RAM and storage, since if 1 team consumes too much resources the other teams will eventually have less and their applications may not schedule

In summary: Namespace uses
=> Structure your components
=> Avoid conflicts between teams
=> Share services between different environments
=> Access and resource limits on namespace level



CHARACTERISTICS OF NAMESPACES:
Before you decide to group or use namespaces:

=> You cant acess most of the resources from another namespace
=> Each namespace must define its own config map; the configMap for projectA in one namespace used to reference a database cant be used in projectB in another namespace
=> The same applies to secrets => the credentials of the shared service, you will have to create the secrets in each namespace 

=> Service is the resource that can be shared between namespaces
=> in addition to  the name of the database the data:db_url will have .namespace_name

Using that namespace you can acces that service from other namespaces

and thats how you can use other services from other namespaces


components which cant be created with a namespace:
These live globally in a cluster and you cant isolate them or put them into a namespace

Example: persistence volumes and nodes, so when you create a volume it is going to be accesible throughout the cluster because it is not in a namespace	and you can list these resources by:
command: kubectl api-resources --namespaced=false

And listing the resorces that are bound to a namesadd


By default if you dont provide a name to a namespace it ceates them in a default NS , so even if you dont specify the namespace when reating or getting the components the -n default is passed down by default
if you do 
command: kubectl get configmap -o wide => you ge the comprehensive version and if you do 
command: kubectl get configmap -o yaml => you will see the namespace of the file is default

if you want to create the config file in your namespace:
command: kubectl apply -f my-config-file.yaml --namespace=mynamespace

Now if you want to get the file you created in a particular namespace, you will do the following:
command: kubectl get configmap -n my-namespace => the -n flag 

Its recommended to use the configuration file for creating namespaces instead of creating it over the kubectl cmd and this will document it better and if you are using automated deployment where you are just applying
the configuration file then it is going to be much important, very convinient way to do it

Change active Namespaces:
Dream scenario: One team gets to work in their own namespace then it could be pretty annoying to add this nametad to every command(-n my-namespace), 
For more convinience it can be changed to whatever namespace you choose


Kubernetes dont have that out of the box but there is a tool called KUBENS
After install running the command kubens will list the available namespaces and  highlight the active one
and to change is simple:
command: kubens my-namespace and that will switch the active namespace

Kubectx is what will install kubens, follow the guide for your system


2:01:50
kUBERNETES INGRESS EXPLAINED:
External service vs ingress:
You can access the ui in the browser using the external service, Where everything will
be accessed via the public ip and the port, This is good for testing but this isno
how the final product should look with the ip adresses but with the domain name  and 
also you want the secure connection using https

That is accomplished using k8s component called kubernetes ingress 
so instead of external service you will have my-app-ingress and instead if external se
nal service you will have an internal service. So you wont open your application ip
adress through a port

Now if you have request from the browser it is gonna reach first to ingressand that 
will be redirected to the internal service and eventually to the pod


External configuration for ingress vs ingress
For the external service you have the kind: service which is of type:loadBalancer which means
we are opening it to public via the ip adress; assign external IP adress to service
and then you will provide the node port where the user will be able to access the application



For ingress:
kind: ingress
then you have routing rules, and this basically defines the main adress order requests
throug that host must be fowarded to the internal service

the path you provide mean the URL path, and everything after the / in the url will be 
defined by you, we will see the examples if the path configuration later

You can use ingress to configure the https connection

the http: key in the ingress config file doesnt correspond to the http:// in the browser


That in the file is the protocal that incoming service gets fowarded to the internal service

Backend is the target where the incoming requests wil be redirected to, and the serviceName key in 
the yaml file configuration shoul correspond to the name of the service in metadata, and the servicePort 
should correspond to the intenal service port (in the internal ports key)

The difference between the external and internal service is that in internal service you dont have the third port
in internal service the one called nodePort which starts from 30000 and the type is default type(clusterIP) not 
instead of Loadbalancer

Host: my-app.com => This should be a valid domain name
and you should map that domain name to the node's ip adress which is the entry point


If you create that ingress component alone wont be enough for ingress routing rules to work, what you need in 
addition is an impelementation for ingress, and that impelementation is called an ingress controller

Step one will be to install an ingress controller

=> Which basically is a pod or set of pods running in your kubernetes cluster and thus evaluation and processing 
of ingress rules

What is ingress controller?
The function is to evaluate all the rules you have defined in your cluster and this way to manage all redirections

=>evaluates all the rules
=> manages all the redirections
=>This will be the entry point to the cluster for all the domain or subdomain rules you have configured
and this will evaluate all the rules because you may have 50 rules or 50 components created in your cluster it will 
evaluate all the rules and decide based on that which fowarding rule applies for that speciefic request

In order to install these impelementation to your cluster you need to decide which impelementation you want to choose from 
the whole link in description

The one from k8s itself called K8s Ingress controller

IMPORTANT TO UNDERSTAND: Interms of setting up the cluster, consider the environment in which the cluster is running
if you are using cloud provider like aws etc that have out of the box k8s solutions or they have their own virtual
their own virtual load balancer,Your flow will look like this:

cloud load balancer(the external request will first hit this) => Will direct the request to the Ingress Controller
This isnt the only way even in cloud there are different ways to impelement this but this is one of the common strategy

The advantage of this is that you dont have to impelement the load balancer yourself, with minimal effort using cloud providers you have 
the load balancer set up in no time and have it receive and redirect the requests to your Ingress Controller pod

In the bare metal infrastructure:
You will have to impelement that part yourself, You will have to configure the entry point to your kubernetes infrastructure yourself

Different ways to do it(Link in the description)

Generally speaking, either inside the cluster or outside as a separate server, you will have to provide an entrypoint and one of those type is an external proxy server
Which can be a software or hardware solution that will take that role of a load balancer and an entry point to your cluster

Basically this means you will have a separate server, and you'd give this a public ip adress and open the ports in order for the requests to be accepted 
and this proxy server will act as an entry point to the cluster

And this will be the only one accesible externally and none of your servers will be accesible outside the cluster, which is a good security practice

request from browser=> proxy server => Ingress controller(chacks the ingress rules, will check which ingress rule applies to that speciefic request) => internal service fowarding will happen



Ingress controller in Minikube:
firstly install ingress controller in minikube
command: minikube addons enable ingress
	Tick => Ingress was succesifully enabled

Automatically starts the k8s Nginx impelementation of Ingress Controller, this is one of many third-party impelementation which you can safely use in production environment
and not just minikube

if you
command: kubectl get pods -n kube-system
you will see the Nginx-ingress-controller running in your cluster

Once you have that installed you can create an ingress rule that the controller can evaluate


Namespace= same service and a pod

after applying the ingress you can get it and you will have to wait for the ingress to be assigned an ip addr
command: kubectl get ingress -n kubernetes-dashboard --watch(if you want to watch for when the ip adress is assigned)

in /etc/hosts you can configure your mapping for the name and ip adress
command: sudo vim /etc/hosts then add your mapping
132.44.34.233: dashboard.com

This mapping works locally


Ingress default backend:
this can be seen using the describe command providing the name and namespace
This can be used to define custom error messages when page isnt found, when request come in you can't handle

so that users can see some mini cool pages or some custom page you can redirect to the homepage
or something like that

All you got to do is create an internal service with the same name default-http-backend and the port number also create a pod or application that 
that sends that custom error message's response

You can define more routing granular for your applications inside the kunernetes cluster


1st use: definining multiple paths for the same host
All those defined paths are like this http/:www.hostname/pathname

2nd usecase: Multiple domains or subdomains
So in case of this configuration will have multiple hosts, where each host represents a subdomain and inside you have 1 path which redirects to 
a partcular service


CONFIGURING TLS SERVICE:
The only thing you have to configure is define tls: key above the rules section with hosts and secretName: a reference which you will have to 
create in a cluster that holds that tls certificate 

In the secrets you will have tls.cert and tls.key of which are both base 64 encoded


also when you create a tls certificate make sure you include the type to be tls
type: 

3notes to be made here:
=> Data keys need to be exactly "tls.crt" and "tls.key"
=> Values are file contents and not file paths or locations
=> You have to create the secrets in the same namespace as the ingress component for it to be able to use that
otherwise you cant reference a secret from another namespace

