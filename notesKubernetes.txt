Kubernetes was developed by google, it manages containers

Open source container orchastration tool 
Helps you manage them in different environments -> like physical machiners, cloud machines or vms

 Managing thousands of containers spread through different environments throgh scripts can be tedious

also the trend from monolith to microservices resulted into the use of thousands of containers

Hence the increased demand for the proper way of managing those hundreds of containers

Tools like kunernetrs guarantee the folloeing:
=> High availability no downtime
 -> scalability and high performance
=> disaster recovery || back up and restore

K8 components explained:
Node => Is a simple server or a virtial machine

Pod => is a smallest unit of k8s
	it is an abstracion of a container, what pod does is create these running environents or a layer o	on top of a container, and the reason is kubernetes wants to abstract away the container runtime and 	container technologies so that you can replace them if you want to and because you dont want to work d	directly with docker or container technology you use in kubernetes so that you only interact with kubernetes layer
	 Pod us meant to run 1 container inside it 

'
Kunernetes offers a virtual netwoek out of the box which which means each pod gets its own ip adress
the pod gets its own ip adress not the container and each pod can communicate with each other using and jip adress which is an internal ip adress and not a public one, so ,my container can communicate with the database using an ip adress 

Note: Pods in kubernetes are ephemeral meaning they can die very easly

and in case of anything going wrong the pod will die and the new one will be creates and a new adress will be created and assigned a new ip adress, This makes it inconvinient as if comminication was through ip asress if the pod dies then the one cretes will lose communication with the newly created pos as they have different ip adress

due to that another component of kubernetes called service is used


SERVICE and INGRESS:
Service is a permanent ip adress that can be attached to each pod so my-app will have its own service and pod will have its own service, the lifecycle of a pod and a service are not connected so even if the pod dies the ip adress will stay 

External service is a service that opens the services to the external sources

Insteas of the esternal request going directly to the intenal services they will fitstly go to ingress then the fowarded to the internal services

CONFIGMAP and SECRETS:
configmap is an external configuration to your application eg urls of the databases etc, also you can put the database username and passwords but this would be insecure

secrets: its just like config map but used to secure secret data
They are not stored in plain text but in base 64 encoded format

The built in security mechanisms are not enabled ny default

You can use this as an environmenal variable or a proprty value


VOLUNES:
Data stprage and storage:
How it accomplishes this is by attaching the actual phydical drive to your pod which means it can be on the same local machine which means the same pod is running ot it could be a remote storage outside of a kubernetes cluster, it could be cloud storage or on premise storage which is not a part of kubernetes structure	you just have a reference to it

 KUbernetes doesnt hanhle data persistence so you hace to handle it yourself 


DEPLOYMENT and STATEFUL SETS:
IF my pod dies then i will have downtime which is a bad thig as my site wont be accesible

A service has two functionalities:
=> provides permanent ip adress
=> is a load balancer -> will catch a request and foward it to the the service which is least busy

To create a new pod you woulnt create another pod but define a blueprint and specify the number of pods you want to run. That blueprint is called deployment.

And you wont be working with pods you will be working with deployments because there you can specify the number if pods you want ans you can scale up or down number of replica of pods you need.

Note: Pod is layer of abstraction on top of containers and deployments are a layer of abstraction on top of pods

Now if one replicas of your application dies the service will foward the request to another one so that the application will be accesible to the user 

You cant replicate database using deployments, reason being database has state meaning its data this means if we have the replicas of database they will have to access the same storage and you will need some mechanisms to know which services are writing to that storage and which are reading from what storage in order to avoid data inconsistencies

deployments are for stateless apps
statefulSet for stateful apps or databases => the reads and writes for the databases ara synchronized

Deploying stateful sets can be tedious and thats why its common to host the stateful applications outside the container ans the stateless ones inside as they easly scale withought having to worry about data losses

SUMMARY OF KUBERNETES COMPONENTS:
=> Pod - abstraction of containers
=> Service - communication between pods
=> Ingress - used to route traffic into the cluster
=> External configuration usind Configmaps and Secrets
=> Volumes - handles data persistence
=> Blueprints using deployments and statefulSets

These are te core or the basic concepts, just using these you can build pretty powerful kubernetes cludters

K8 architecture explained:
Master processes
Each container will have multiple pods running in them 
-> three processes must be installed on every node

Worker nodes are the nodes that actually do the work, hence the name

The first proccess that need to run on every node is a container runtime
the processes that schedules the pods and the containers underneath is KUBELET
which is process of kubernetes itself unlike container runtime
Kubeletes interacts with both the node and the container 

Kubeletes starts the pod with the container inside and assigning resources to the container like cou ram and storage resoutces 
Usually kubernetes cluster is made up of multiple nodes which must have container services and kuberkets installed 

The third service that is respnsible for fowarding requests from services to pods is kube proxy that must be installed on every node and kubeproxy have an intelligent fowarding inside that make sure the indoemation works in the perfotmant way with low overhead For example if my-app is make request to database the request is not fowaeded to a random database but the satabase pod inside the same pod and hence avoiding the netwoek overhead of sending the request to the other machine

SUMMARY: 3Processes
=>Kubelet
=>Kube proxy  -these two must be installed on every node
=> container runtime


INTERRACTION WITH THE CLUSTER:

How to schedule a pod?
monitor?
reschedule or restart a pod?
join a ner node?

All these processes are done by master nodes: 4 of these process are:


=> Api server -> is like a cluster gateway its a way you can interact with the woeker nodes using kubernetes dashboard or kubelet cli
it acts as a gate keeper  for authentication to make sure only the authenticated users gets to the cluster

if you have a request => API server => Validates request => other processes => pod 
and if you eant to query the cluster health or the status of your deployment you make the request throgh the API server and it gives you the response 

2ndly: Scheduler
Schedile a new pod: API server -> scheduler => pod(the secheduler is intelligent to decide on which node pod will be scheduled or bext component will be scheduled)

Firstly it will look at your request and see how muvh of the resources you need and its gonna go through the woeker nodes and see the availavle resources in each and one of them, on the least busy is when the pod will be scheduled

TIP: Scheduler decides on which node the new pod will be scheduled, the process that actually start that container is a cubelet 


#rprocess: Controller manager
It detects state changes like crashes, and tries to recover the cluster state as soon as possible 

so contoller manager=> scheduler => Kubelet


4thrly etcd
This is like a cluster brain 
Cluster changes are stored in the key value store, and the reason its a cluster brain is because all these mechanisms with controller etc woek works because of the etcd data
For example how does resources available on each worker node, did the cluster state change? is the cluster healthy? all these are answered by having the info from etcd cluster

what is not stored:
The actual application data
etcd just storest the cluster information used for master processes to communicate with the work processes and vice versa

These master processes are crucial and each master node runs its master processes where the API server is load balanced and etcd stores distributed storage across all the master nodes

The actual hardware of master and worker resources differ, master nodes are importance but the have less load od work so they need less resources where the worker nodes do the actual work to  run those containers and therefore they need more resources and as the application scales you can add more nodes to meet the required application resources, so if you need the extra master nodes you get the bare metal server install the master processes and add it to kubernetes cluster
 and the same applies if you need two woeker nodes you get two bare metal servers install the woeker node processes and all it to kubernetes cluster and this way you can infinitely increase the power of kubernetes cluster as it replication level and resource demand increases


MINIKUBE and KUBRCTL:
Production cluster setup
-> it will have multiple masters atleast two in production setup 
-> and multiple worker nodes

minikube is this one test/local cluster setup where it is a one node cluster  when master and worker processes both run on the same machine and this will have docker runtime pre-insalled

so this minikube will create a virtual box on your laptop
=> node runs in that virtual box 
=> 1 k8s cluster

which can be used for testing purposes 
now you have a cluster you need a way to interact with your cluster and thats where this comes in:

KUBECTL:
kubectl is a commandline tool for k8 structure

Note: Minicube runs both master and worker processes

You first have to interact with the api server to do any kubernetes work
 so to interact with Api server can be through api, ui or cli of which all of these are all clients

and kubectl is the most poweful of all the clients


IMPORTANT: kubectl isnt just for minikube clustet 	if yoou have a cloud cluster or hybrid cluster minikubectl is the one to interact with the setup

!!!!Since minikube uses virtualization hence virtualization has to be enabled in your machine
 and you need to install some tyoe jof some hypervisor 

COMMANDS:
Minikube start => Creates and start the cluster
 options --vm-driver=hyperkit

kubectl get nodes => gets status of nodes

minikube status => also gets you the status of the running kubes

kubectl version => gives the version of installed kubernetes -> this will show you the client and the server version of this 


IMPORTANT:
Minikube CLI is for starting or deleting the cluster

Kubectl CLU is for configuring the minikube cluster

BASIC KUBECTLCOMMANDS:
Prequesities:
Have minicube cli and kubectl installed in your system 

You are going to be using kubectl for the control of anything

commands: 
kubectl get nodes
kubectl get pods
kubectl get services

to create anything kubectl create -h will give you the man pages for creating

In creating there is no pod option, there is an abstraction and that is deployment and that will create pods underneath

to create the creation is made from image and this is the command:
kubectl create deployment nginx-depl image=nginx

-> this will download the latest image of nginx from docker Hub

To see the seployments you can do : kubectl get deployment and if you want to get the pod: kubectl get pod

when i create the deployment it is the blueprint for creating the pods and the most basic configuration is providing the name and the image 


Between deployment and a pod there is another another abstraction layer called replicaset which is autimatically managed by kuberneter deployments 

command to get the replicaser: kubectl get replicaset

the podname has the prefix of deployment and replicaset is and its own id

the replocaset is basically managing the replicas of a pod, You wont be manually required to create or delete a replicaset file but you gonna be working with deployments directly  to create multiple replicas you can provide te options to do that 


check this out:
a deployment manages a:
replicaset manages a:
a pod is an abstraction of 
a container 

 thats the hierrachy and everything below the deployment should be managed by kubernetes

to edit:
kubectl edit deployment [name] => this will give you the auto generated configuration file with default values because at the command if you gave name and an image everything else is auto generated

and if you edit it will open the text editor and yoou can change some configurations and after saving the info the old pod eg. if you changed the image version will be terminated and the new one will be start running 

51:37


kubectl get replicaset: => this will show the old and the new replicas of the same pod


DEBUGGING LOGS:
kubectl logs <logname> -> shows what the application inside the pod actually logged

describe gives you more information about the pod 
command: kubectl describe [pod name]

then run kubectl get pods

and to see the logs => kubectl logs [pod-name]

to get inside the terminal of the particular container or application
command: kubectl exec -it [podname] --/bin/bash

Deleting a deployment and Applying the configuration file:

To get the deployments: kubectl get deployment

IMPORTANT: deleting the deployment will delete the replicasets and the pods 
command: kubectl delete deployment [deployment name]

IMPORTANT: All the crude operations happens at the deployment level and everything underneath falls automatically

When you create the deployments you should provide the options like:
--name image option1 option2 and lots of other things you would want to configure and it would be impractical to write all that out on a commandline, so because of that in practice you would be working with the configuration files

The way to do it is to use the apply command which takes in the configuration file

command: kubectl apply -f [configuration-file.yaml]

You make the configuration file and then apply tha and ithat will take care of creting the deployment 

Summary:
create a deployment => kubectl create deployment [name]
Edit deployment => kubeclt edit deployment [name]
Delete deployment -> kubectl delete deployment [name]

status of different k8s components:
kubectl get nodes | pod | services | replicaset| deployment

Debugging pods:
log to a console => kubectl logs [pod name]
Get interactive terminal kubectl exec -it[podname] --bin/bash
Get info about pods=> kubectl describe pod [pod name]

Use configuration file to create deployments:
Apply a configuration file  => kubectl apply -f [file name]
Delete with a configuration file => kubectl delete -f [file name]

K8s yaml files:
composed of three main parts:
Specifications
Metadata
Version and type of component

the kind of specification will be speciefic to the kind of a component you are creating

The deployment will have its own attribute that only apply fot deployment and service will have its own stuff

The third part of the yaml file is status and will be automatically generated by kubernetes

Status:
K8s always compares what is the desired state and what is the actual state

and if the desired state do not match then k8s knows there is something to be fixed
and it is going to try to fix it and this is the basis of self healing that kubernetes tries to provide 

it will continously compare the status with the specifications and woll know there is an issue and will contonously try to fix it 

Where does k8s get the state to update continously: The info comes from etcd 
Etcd holds the current status of any k8s component and thats where the status information comes from 

Format of configuration file:
Yaml is very strict about the identation 

You can use an online yaml validator to see if there is any issue with your syntax

The usual practice is store these configuration files with your code, since deployment and service will be applied to your application code, usually it will be a part of iac infrastracture as code or you can have its own git repo for the configuration files

Layers of abstraction:
Deployments manages a:
Replicaset manages a:
Pod is an abstraction of:
Container

A pod shoul have its configurations inside the deployments too



Connecting components(Labels, selectors and ports):
Metadata contains the labels and specification part contains selectors

In metadata yoou give components value pairs and that label sticks to that component
Pod get the label through template blueprint
This label is matched by the selector 

To get the more information about the pod: kubectl get pod -o wide

kubectl get deployment nginx-deployment -o yaml and to save it > nginx-deployment-result.yaml it will be saved and you can make a comparison and try to see the added status  and other things added by kunernets by defailt


COMPELETE DEMO PROJRCT:
Overview of the components going to be involved:
2 deployment/pod
2 service (create internal service)
1 config map
1 secret

Request flow through the browser
Request comes from the browser,
=> goes through the external service of the mongo express 
=> which will foward it to the mongo express pod 
-> the pod will then connect with internal service of mongo db and is basically the url here  (configmap)
=> and will then fowart it to the mongodb data which will then authenticate the request using credentials\



Firstly have the minicube cluster running

kubectl get all gets you all the kubernetes componetns  that are in the cluster

and for now if you are using minikube then the only running default cube is the default minikube cluster

The first thing is creating a mongo db deployment

You have to refer to the dockerhub repository to get to see the required configurations or the optional configurations that are available

This will show you the default port and the environmental variables

When you define the yaml file its going to be defined in the repostory so its the bad practive to include the username and password there inside the configuration file there its not supposded to be plave there 

The secrets lives in k8s and not in the repository 
the git one 

Befire applying the deployment you first have to create the secrets where the credentials will live

SECREt CONFIG FILE:
Kind -> secret
-> metadata/name => a random name
type -> "opaque default for arbitary key value pairs


Other types include the tls certificate for tls certificate files 
the data with the following syntax will be used for the key value pairs:
data:
	mongo-root-username: the_name
	mongo-root-password: the_passkey

Storing data in secrets component doesnt make it autimatically secure 

There are built in meachanisms like encyrption for basic security  which are not enavled by defauly 

the simple way to do it is:
command: echo -n 'username' | base64

the output will be base 64
and the the same will be applid for the password

All that is preparation work

IMPORTANT: You have to create sectets before the deployment if you are going to reference the secrets in the file you are creating, so the order of creation matters


If you make the deployment of the secrets which doesnt exist yet you are going to get an error

You have to apply the secrets before even applying the deployment otherwise you are goung to get an error

after applying it to see the secrets you can run kubectl get secret  you will see if your secrets have been created

Then after you already have your references you can now reference in your deployments
in the deployments instead of values you have to say values from:
env: 
	name: some-env-variable
	valueFrom: 
		secretKeyRef:
			name:secret-name(The actual name in your file not the name of the file the name in the name section after ApiVersion)
So in the value you will output your key not the base64 actual value

Yaml is very strict with the indetation 

base64 just encodes the given string to remove the non ascii charaters but its not for encryption for actual encyrption you have to use tools like hascorp vault

But this encoding is still good since you dont want your code in your repository 

note: to apply the deployments:
command: kubectl apply -f mongo.yaml

to get everything:
command: kubectl get all 

To see whats going on in the pod you have to decribe the pod
command: kubectl describe pod <podname>

The second step is creating the external service so that the other pod can talk to this mongodb

You can create separate files for yaml files or with --- you can start a new document

Its okay to put deployment and a service in one file since they belong together

The way to connect the service to the pod is using selector  -=> to connect to the pod through the label, by using the label of the pods the service through the labels can find the pods to attach to 

port: service port
targerPort: container port for deployment

service port and targer port have tp match, They can be different but under what conditions??
ChatGPT said:

Great question! In Kubernetes, Service port and targetPort can be different, and this is actually a common and useful feature. Let me break down what each means and when you’d want them to match or differ.

What are port and targetPort in a Kubernetes Service?

port (or service port):
This is the port that the Service exposes to other pods or clients inside the cluster (or outside, if it's a NodePort or LoadBalancer service). It's the port clients use to connect to the service.

targetPort:
This is the port on the pod/container that the Service routes traffic to. It’s the actual port the application inside the pod listens on.

When do they need to match?

When your Service’s exposed port is the same as the container’s listening port.

Example:

spec:
  ports:
  - port: 80
    targetPort: 80


This means the Service listens on port 80, and forwards traffic to pods on port 80.

When and why can they be different?

If your application listens on a different port than what you want exposed externally.

Example:

spec:
  ports:
  - port: 80
    targetPort: 8080


Here, clients connect to the Service on port 80, but the Service forwards requests to port 8080 on the pod.

Why would you do this?

Abstraction:
Clients don’t need to know the actual port your app listens on inside pods. You can expose a standard port externally (like 80 or 443) regardless of the internal pod port.

Multiple ports:
Your pod might have multiple containers or multiple ports open, and the Service selectively exposes some.

Port remapping:
You may want to unify or change the exposed port without changing the app’s container port.

Additional notes

If targetPort is not specified, it defaults to the same value as port.

The targetPort can be specified either as a number or as a string referring to the port name defined in the Pod spec.

Example with named ports:

Pod Spec:

containers:
- name: app
  ports:
  - containerPort: 8080
    name: http


Service Spec:

ports:
- port: 80
  targetPort: http  # refers to containerPort named "http"

Summary
Condition	port	targetPort	Explanation
Simple, same port exposed	80	80	Service port matches container port
Expose different port externally	80	8080	Service port is 80; pod listens on 8080
Use named ports	80	"http"	targetPort refers to named containerPort
targetPort omitted	80	80 (default)	targetPort defaults to port value

If there are multiple k8d components defined in the same yaml file then the k9s will know which has not changed and what has changed when you apply them

If you edit both then both deployment and service will be changed

to see what port is connected to what port then you can run kubectl describe service and will list the target port and you can see the ports connection or mapping

to get extra information about the pos and to what service is connected to you can:
command: kubectl get pod -o wide

To get all the componets you can:
command: kubectl get all and you can grep them by name and get the exact pod you are looking for 

Next we are going to configure the mongo service and mongo express and create configmaps where the satabase utl is gonna be 


For the place to add the adress of the mongo database you can add it to the yaml file directly or you can use the configMap

configMap:
=> external configuration
=> centralized
=> Other components can use it

and then you can reference that external configuration in that file when the configuration is needed

configMap configuration file:
kind: configMap
metadata/name: a random name

data: which is the key value pairs
server name is the name of the service and its simple as that and that is going to be the database name url


Just like the secrets the order of execution matters, you have to apply the config files before you can apply them: IOW the configMap must be already in the cluster before you can reference it


The way you can reference the config maps inside the deployment is very similar to secrets and now instead of configMapRef its now gonna be configMapKeyRef and as it was for the secrets the mane is the actual name that was used in the name key from the configuration yaml file



External service to access Mongo from the browser

In practice you never have service without deployments so it makes sense to put them together

The external service file is declared the same way as the service even the kind of service, so how do you make this service external?
so below the selector im gonna put a type and the type of this is loadbalancer, (But internal service also is a load balancer so its kind of a bad name for sure)

What the load balancer type of service does is it assigns an external ip adress and so accepts external requests

also you will provide the node port:
This is the port where this external ip adress will be open, That will be the port for external ip adress and the port you wull put into your browser


AND THat must be between 30K to 32767

Internal service would not apply to any type

internal service(Cluster IP) is default; so you dont have to define it when creating the internal service, While the Loadbalancer type will have to be defined for an external service explicitly 

and in the external ip adress inside the minikube shows up as pending but for the real machine the public ip adredd will show up


minikube service service-name => This will assign external service an external ip adress

CHAIN OF COMMAND:
Browser request => Mongo express service => Mongo express pod -> Mongodb internal service => Mongodb pod 

KUBERNETES NAMESPACES EXPLAINED:
What is a namespace:
In k8s you can organize the cluster in namespaces, and you can have multiple namespaces in k8s cluster
=> Think of it like a virtual cluster inside the cluster

If you create a cluster the k8s gives you 4 namespaces by default
if you type: 
command: kubernetes get namespace you see the list of the namespaces
Explanation:
Kuberneters-dashboard only ships with minikube, it is speciefic to minikube installation, you wont have this in a standard cluster

kube-system =>> this namespace is not meant for your use, shouldnt name anything or modify anything in kube-system

the components which are deployed in kubesystem namespace are the system processes, master and kubectl processes

kube-public =>> contains the publically accesible data, It has a configMap that contains cluster information which is accesible even without authentication 
if you type kubectl cluster-info this will output the configurations


kube-node-lease =>> Which is the recent to kubernetes
Purpose:
Holds the heartbeat of the nodes, each node has its associated lease in namespace
determines the node availability

Fourth name is the default namespaace:
=> Will be used to create the resources that you are going to need, or the resource you create are going to be located there

You can create your own namespace 
command: kubectl create namespace <your_namespace>
to list them: kubectl get namespace

Another way is to use the namespace configuration file which is the better way to so that 

WHY USE NAMESPACES:
The best use of namespaces is to group the resources for instance you can have the database namespace and have inside of it all the required resources, Ypu can have monotoring namespaces where you deploy all the prometheous, and all the stuff it needs , You can have a namespace where the elastic search stack like kibana lives,  then you can have the other for nginx-ingress resources

This is just one way of grouping your resources inside of the cluster

According to the official k8s doc you shoulnt use namespaces if you have smaller projects, up to 10 users, But in general it is a good idea to group your namespaces

EVEN IN MINIMUM PROJECT USE NAMESPACES

Annother usecase scenario is when working in a team and all of them use the same deployment and would result into overriding the original file and can result into issues


another use case scenarion is if you want to host both the stating and development environment in the same cluster and the importance of this it will make it easier in the usage of environments which are common to both



another use case is when using the blue/green deployment, which means that in the same cluster and you want to have two versions of production, the one which is active now and the other one which is in the production next
The versions of the apps in the blue and green will differe but these might be in different namespacces but they will need to use the same resources, and in this way they can use the common shared resources without having to set up a separeate cluster


The other use case is to limit the acess and resources in the namespace, when working in teams, so that this way they are able to perform the crude operations in their own namespaces but cant do anything in any other namespace, this can minimize the risk if interfearences between teams. Each team has their own secure isolated environment

The other thing is you can limit the resources each namespace consume eg. CPU, RAM and storage, since if 1 team consumes too much resources the other teams will eventually have less and their applications may not schedule

In summary: Namespace uses
=> Structure your components
=> Avoid conflicts between teams
=> Share services between different environments
=> Access and resource limits on namespace level



CHARACTERISTICS OF NAMESPACES:
Before you decide to group or use namespaces:

=> You cant acess most of the resources from another namespace
=> Each namespace must define its own config map; the configMap for projectA in one namespace used to reference a database cant be used in projectB in another namespace
=> The same applies to secrets => the credentials of the shared service, you will have to create the secrets in each namespace 

=> Service is the resource that can be shared between namespaces
=> in addition to  the name of the database the data:db_url will have .namespace_name

Using that namespace you can acces that service from other namespaces

and thats how you can use other services from other namespaces


components which cant be created with a namespace:
These live globally in a cluster and you cant isolate them or put them into a namespace

Example: persistence volumes and nodes, so when you create a volume it is going to be accesible throughout the cluster because it is not in a namespace	and you can list these resources by:
command: kubectl api-resources --namespaced=false

And listing the resorces that are bound to a namesadd


By default if you dont provide a name to a namespace it ceates them in a default NS , so even if you dont specify the namespace when reating or getting the components the -n default is passed down by default
if you do 
command: kubectl get configmap -o wide => you ge the comprehensive version and if you do 
command: kubectl get configmap -o yaml => you will see the namespace of the file is default

if you want to create the config file in your namespace:
command: kubectl apply -f my-config-file.yaml --namespace=mynamespace

Now if you want to get the file you created in a particular namespace, you will do the following:
command: kubectl get configmap -n my-namespace => the -n flag 

Its recommended to use the configuration file for creating namespaces instead of creating it over the kubectl cmd and this will document it better and if you are using automated deployment where you are just applying
the configuration file then it is going to be much important, very convinient way to do it

Change active Namespaces:
Dream scenario: One team gets to work in their own namespace then it could be pretty annoying to add this nametad to every command(-n my-namespace), 
For more convinience it can be changed to whatever namespace you choose


Kubernetes dont have that out of the box but there is a tool called KUBENS
After install running the command kubens will list the available namespaces and  highlight the active one
and to change is simple:
command: kubens my-namespace and that will switch the active namespace

Kubectx is what will install kubens, follow the guide for your system


2:01:50
kUBERNETES INGRESS EXPLAINED:
External service vs ingress:
You can access the ui in the browser using the external service, Where everything will
be accessed via the public ip and the port, This is good for testing but this isno
how the final product should look with the ip adresses but with the domain name  and 
also you want the secure connection using https

That is accomplished using k8s component called kubernetes ingress 
so instead of external service you will have my-app-ingress and instead if external se
nal service you will have an internal service. So you wont open your application ip
adress through a port

Now if you have request from the browser it is gonna reach first to ingressand that 
will be redirected to the internal service and eventually to the pod


External configuration for ingress vs ingress
For the external service you have the kind: service which is of type:loadBalancer which means
we are opening it to public via the ip adress; assign external IP adress to service
and then you will provide the node port where the user will be able to access the application



For ingress:
kind: ingress
then you have routing rules, and this basically defines the main adress order requests
throug that host must be fowarded to the internal service

the path you provide mean the URL path, and everything after the / in the url will be 
defined by you, we will see the examples if the path configuration later

You can use ingress to configure the https connection

the http: key in the ingress config file doesnt correspond to the http:// in the browser


That in the file is the protocal that incoming service gets fowarded to the internal service

Backend is the target where the incoming requests wil be redirected to, and the serviceName key in 
the yaml file configuration shoul correspond to the name of the service in metadata, and the servicePort 
should correspond to the intenal service port (in the internal ports key)

The difference between the external and internal service is that in internal service you dont have the third port
in internal service the one called nodePort which starts from 30000 and the type is default type(clusterIP) not 
instead of Loadbalancer

Host: my-app.com => This should be a valid domain name
and you should map that domain name to the node's ip adress which is the entry point


If you create that ingress component alone wont be enough for ingress routing rules to work, what you need in 
addition is an impelementation for ingress, and that impelementation is called an ingress controller

Step one will be to install an ingress controller

=> Which basically is a pod or set of pods running in your kubernetes cluster and thus evaluation and processing 
of ingress rules

What is ingress controller?
The function is to evaluate all the rules you have defined in your cluster and this way to manage all redirections

=>evaluates all the rules
=> manages all the redirections
=>This will be the entry point to the cluster for all the domain or subdomain rules you have configured
and this will evaluate all the rules because you may have 50 rules or 50 components created in your cluster it will 
evaluate all the rules and decide based on that which fowarding rule applies for that speciefic request

In order to install these impelementation to your cluster you need to decide which impelementation you want to choose from 
the whole link in description

The one from k8s itself called K8s Ingress controller

IMPORTANT TO UNDERSTAND: Interms of setting up the cluster, consider the environment in which the cluster is running
if you are using cloud provider like aws etc that have out of the box k8s solutions or they have their own virtual
their own virtual load balancer,Your flow will look like this:

cloud load balancer(the external request will first hit this) => Will direct the request to the Ingress Controller
This isnt the only way even in cloud there are different ways to impelement this but this is one of the common strategy

The advantage of this is that you dont have to impelement the load balancer yourself, with minimal effort using cloud providers you have 
the load balancer set up in no time and have it receive and redirect the requests to your Ingress Controller pod

In the bare metal infrastructure:
You will have to impelement that part yourself, You will have to configure the entry point to your kubernetes infrastructure yourself

Different ways to do it(Link in the description)

Generally speaking, either inside the cluster or outside as a separate server, you will have to provide an entrypoint and one of those type is an external proxy server
Which can be a software or hardware solution that will take that role of a load balancer and an entry point to your cluster

Basically this means you will have a separate server, and you'd give this a public ip adress and open the ports in order for the requests to be accepted 
and this proxy server will act as an entry point to the cluster

And this will be the only one accesible externally and none of your servers will be accesible outside the cluster, which is a good security practice

request from browser=> proxy server => Ingress controller(chacks the ingress rules, will check which ingress rule applies to that speciefic request) => internal service fowarding will happen



Ingress controller in Minikube:
firstly install ingress controller in minikube
command: minikube addons enable ingress
	Tick => Ingress was succesifully enabled

Automatically starts the k8s Nginx impelementation of Ingress Controller, this is one of many third-party impelementation which you can safely use in production environment
and not just minikube

if you
command: kubectl get pods -n kube-system
you will see the Nginx-ingress-controller running in your cluster

Once you have that installed you can create an ingress rule that the controller can evaluate


Namespace= same service and a pod

after applying the ingress you can get it and you will have to wait for the ingress to be assigned an ip addr
command: kubectl get ingress -n kubernetes-dashboard --watch(if you want to watch for when the ip adress is assigned)

in /etc/hosts you can configure your mapping for the name and ip adress
command: sudo vim /etc/hosts then add your mapping
132.44.34.233: dashboard.com

This mapping works locally


Ingress default backend:
this can be seen using the describe command providing the name and namespace
This can be used to define custom error messages when page isnt found, when request come in you can't handle

so that users can see some mini cool pages or some custom page you can redirect to the homepage
or something like that

All you got to do is create an internal service with the same name default-http-backend and the port number also create a pod or application that 
that sends that custom error message's response

You can define more routing granular for your applications inside the kunernetes cluster


1st use: definining multiple paths for the same host
All those defined paths are like this http/:www.hostname/pathname

2nd usecase: Multiple domains or subdomains
So in case of this configuration will have multiple hosts, where each host represents a subdomain and inside you have 1 path which redirects to 
a partcular service


CONFIGURING TLS SERVICE:
The only thing you have to configure is define tls: key above the rules section with hosts and secretName: a reference which you will have to 
create in a cluster that holds that tls certificate 

In the secrets you will have tls.cert and tls.key of which are both base 64 encoded


also when you create a tls certificate make sure you include the type to be tls
type: 

3notes to be made here:
=> Data keys need to be exactly "tls.crt" and "tls.key"
=> Values are file contents and not file paths or locations
=> You have to create the secrets in the same namespace as the ingress component for it to be able to use that
otherwise you cant reference a secret from another namespace


HELM: THE PACKAGE MANAGER OF K8s:
What is helm => is a package manager for k8s, you can think of it like apt, yum or brew for kubernetes

Its a convinient way to package k8s files and distribute them in public and private repositories

For example if you wanted to use elastic search in k8s you would need to set up; statefulSet, 
configMaps, secret, services and k8s user with permissions

and if you were to create these files manually by searching for each and one of them separately on internet would be a tedious job
until we have all these yaml files collected tested would take some time

Since elastic search is standard used by the other people it would make sense, someone created this yaml file and someone packaged this file and made it available 
to other people that use the same kuberneters cluster and that bundle of yaml files is called helm charts


So using helm charts you can:
=> Bundle of yaml files
=> Create your own helm charts with helm 
=> Push them to the helm repository to make it available to others or you can download and use the existing ones, that people
	made available from different repositories

All there database applications and monitoring apps that have this complex setup all have charts available in the helm repository

so using the simple command you can set this up
command: helm install some_chart 


You can reuse the charts that someone has written already and sometimes its even the company that
created the application 
=> This capability of sharing charts is what made helm popular compared to its altenative tools

searching alredy available k8s components you can do:
command: helm search <keyword>

or you can go on public repositories or helm Hub or other repositories that are available


<All the relevant info in the description>


Apart from public repositories there are private repositories
=> This is to share them internally in the organization

and that would make it valid to create a private repository

Another functionality of helm is: Its a templating engine, Imagine having an application made up of s
several microservices and you are deploying them in your k8s cluster, the deployment and the service of each are pretty much the same
where the only difference is the version name and docker tags are different, So without helm you would
write separate configuration for each file
You would have multiple version files each having its own deployment name and version 

Since the difference in those files are just couple of lines and most of the content is just the same 

Using helm you can define a common blueprint for all the microservicesand the dynamic values that are
gonna change replaced by the placeholders

and that would be the template file 
syntax: {{ values }} => that is the placeholder for the values from external configuration

that external addition yaml file called values.yaml and there you can define all the values that
you will use in that template file

what values is in this(.values.parameter.something) is an object created based on the values supplied
by values yaml file or through commandline with --set flag 

This is very practical if you are using CI/CD 
Because what you can do is in your build pipeline you can use those template files and replace the values on the fly before 
deploying them

Another use case where you can use helm features:
Is when you deploy the same set of applications across different k8s clusters

If you have development, staging and production environments you can make your own chart and use them
to deploy across different k8s cluster environment. This can make the whole deployment process easier

Helm chart structure:
Directory structure:

my_chart/
	chart.yaml
	values.yaml
	charts/
	templates/
	...

Chart.yaml is the file that contains all the meta information about the chart
like name, version, dependencies etc.

values.yaml is where all the values for the template files resides, and these will be the default 
values that you can override later

charts folder will have dependencies, that means that if this chart dependencies depends on this 
chart then they will be stored there

templates folder is where the template files are stored
so when you execute 
command: helm install <filename> ==> then template files will be filled with the values from 
values.yaml file

optionally you can have other files in this folder like license file or Readme file  etc.

Values injection into template files:
considering the values.yaml which is the default yaml file; it has these by default: imageName, port and version
 and these default values can be overwritten in couple of different ways:

First: -> When executing helm install	you can provide an altenative values yaml file using values flake 
command: helm install  --values=my-values.yaml <chartname>

The two files will be merged and will result into .values object which will look like this 

Secondly: => You can provide the values using flake where you can define the values directly on the commandline

helm install --set version=3.14.4

 Its better organized and a good practice to provide the values instead of providing them on the commandline



Third feature: Release management
which is provided based on its setup

Differences between version two and three
in version two: Helm comes in two parts Helm client(helm CLI) and the server parts and the server part is called Tiller

whenever you deploy helm chart using helm install <chartname>

helm client will send files to Tiller that has to run in the k8s cluster and tiller will execute these requests 
and create the components from these yaml files inside the k8s cluster

 AND  this architecture offers an important feature called release management

Whenever you sent the new configuration request then helm will send and save a copy of the file for
future reference and thus creating a history of charts executions

so running 
command: helm upgrade <chartname>
 the changes will be applied to the existing deployment instead of removing it and creating a new one

incase some files or configurations are wrong or there are unwanted issues you can roll back to the previous version
command: helm rollback <chartname>

All this is possible because of the charts history that helm keeps when you send the requests to the server from the helm cliernt

The downsides of tiller:
Tiller has too much power in the k8s cluster, it can perform all the CRUDE operations and it has too much permissions
=> This makes it a big security issue and this is why in helm3 they removed the tiller part and its just the helm binary now

and solves the security concern


K8s VOLUMES EXPLAINED:
Persist data in k8s using volumes:
Persistent volume
persistent volume claim
Storage class

When you restart the pod all the changes you have made in the database is gone

because k8s dont give you data persistence out of the box

Its something that you have to explicitly have to configure for each application that requires to persist 
data between pod restarts
You need storage that doesnt depend on the pod lifecycle and it will still be there when the pod dies and the 
new one gets created, so that the new one can pick up where the old left off and it will read the data from the 
the database to get up to date data

However you dont know on what node the pod restarts, so your storage must be available on all pods and not just o
one speciefic, so that when the new pod tries to read the data the up to date data is there


You will need the high available storage which will survive even if the whole cluster crashes


Those are the criterias that your database will need in order to be reliable



persistent volume:
This of pv as a cluster resource, just like ram or cpu
created just like othe k8s components using yaml files 
=> you have to define different parameters just like how much storage,

Since pv is an abstract component it must take an actual physical storage like local hard drive  from the cluster nodes
or external nfs servers outside te cluster or may be cloud storage or from GCP

Where does this storage come from and who makes it available to the cluster || and thats the tricky part of data persistence in k8s


Because k8s doesnt care about your actual storage, it gives persistenceVolume component as an interface to an actual storage that you as
the maintainer/administrator you have to take care of.
so:
=> You have to decide what type of storage your cluster would need
=> Yon need to create and manage them by yourself

Managing meaning you need to do backups and make sure they are not corrupt etc.

Think of k8s pv as an external plugin to your cluster

Whether its a local or remote cluster you are managing

The storage can be local, nfs servers or cloud storage

Depending on the storage type some of the spec attributes may differ because they are speciefic to the 
storage type

In the k8s official documentation you can see the list of the volumes k8s support

NOTE: Persistent volumes are not namespaced, Meaning they are accesible to the whole cluster and unlike
other components that we saw like pods, services they are not in any namespace and they are available to the whole cluster and all the namespaces

categories of pvs:
Local vs remote namespaces:
Each one has its usecase

Local volumes violate 2 and 3 requirement for data persistence;
xxx=> Not being tied to 1 speciefic node -> to each node equally b'se you dont know where the new
pod will start 

xxx-> Surviving in cluster crash scenario, Because of this you should always use remote storage


Who creates these persistence volumes and when?
Pv are resources like ram and cpu so the have to already be there BEFORE when the pod that uses the
storage or depends on it is created	

system admins creates the pv and the users will have  to figure out how to manoally create the configuration files to use the pc
that the admin creates using the yaml configuration file

Types of storage used in k8s refere to the doc

its recommende to use remote storage as the local storage breaks two pv rules; which are availability from everywhere and 



The k8s user user ta\hat deploys the spplication in the cluster either directly or theough ci pipeline

The administtrator would be the one to configure this storage, Make sure the nfs server is there and configured or may be create and configure cloud
storage that will be available for the cluster and then secondly create pvs components from this storage backends

Get the info from develepers team on what type of storage their application would need and developers will know that storage is there and can be used 
b their application

But the developers will have to configure the yaml files in order to use those pvs components; In other words the applocation have to claim
that volume storage and you do that by using another component of k8s called pv claim which are also created by using yaml configuration too


The highlevel of how this woeks is pvc claims the volume with certain storage sizw which is defined in the pv claim and some additional characteristics like
access type and whatever pv sastifies these claims will be used


After that you have to use that claim in yout pods configuration, In your pod you have the volume value that references the PVC with 
its name

Note: Pods access the storage by using the claim as a volume, they request a valume through a claim , the claim than will go and try to find the pv in the 
cluster that sastifies the claim and the claim will have the actual storage backend that it will create that  storage resource from and this way the pod
will be able to use that actual storage backend 


Note: Claims must exist in the same namespace as the pod using the claim

Once the pod finds the volume through pvcl, the volume is then mounted(through a label check if this is true) and if you have multiple containers you can 
decide whether to maount this storage in all containers or just some of those
+++sss>> Now the container can raead and write inside that storage and when the pod dies and new one gers created it will have access to the same storage
and see all the changes the previous pods or previous containers made



Why so many abstractions?
Admin creates the pc => user creates claim to the pv

As a user wanting to deploy your application to the cloud you dont care where the actual storage is, you know you want your storage to have persistence 
and the storage being on aws gcp or anywhere doesnt matter as long as the data is safely stored




These 2 volume types are different from the rest and should be mentioned separately:
ConfigMap and secrets
=>Both are local volumes,
=>These two arent created via pv ot pvcs

But rather their own components and managed by kubernetes itself

Consider a case where you need a configuration file foe your prometheus pod or your messages brocker service

like mosquirto  or when you need a certificate file mounted in your applocation:
In both cases you need a file available to your pod  and how this woek is:

=> You create a configMap or secret component
=> You can mount that in your pod or container the same woy you woul mount pvs to your pv


Summary of what we have covered so far:
volume is just a directory with some data
The volumes are accesiboe in containers in a pod
How the directories are made available or backed by which storage medium are defined by speciefic 
volume types(refer the documentation)


To use the volumes the pod specifies what volume to provide for the pod in the specification volume
attribute and inside the pod you can decide where you can mount that storage in that container
Using volume mounts attribute in the container section

Apps can access mounted data here in the MountPath(key also attribute)


A pod can use different volumes of different types simulatineously 
For instance you have an elastic app that uses certificates, aws elastic storage and the configuration 
file storage then you can configure all three inside your pod or deployment

In the volume mounts you can list all the storage mounts using their names

To persist storage in k8s admins need to configure storage for the cluster
Create persistent volumes and then developers can claim that pv using pvc

Taking a scenario where multiple containers are deployed daily then the developers need to 
ask the admins for the storage so that the admin can manually request for more storage from the
cloud or storage providers and create hundreds of pv  for all the applications that need storage manually 
that can be tedious, time consuming and can get messy quickly

So to make this process more efficient; there is a third component of k8s called storage class sc provides
persistent volumes dynamically; when ever a pvc claims it and this way creating or claiming storage in acluster
is automated

Storage class also gets created using yaml configuration file; example:
kind: StorageClass
=> sc creates pv dynamically in the background 

REMEMBEr: we define the storage backend in the persistent volume component and now we have to define it in the
in the storage class component and we do that using the provisioner attribute, which is the main part of the cluster
configuration because it tells k8s which provisioner to use for a speciefic storage platform cloud providerto create the pv 
components out of it

So each storage backend has its own cloud provisioner that k8s offers internally prefixed with -"kubernetes.io" and there are internal 
provisioners and for the external provisioners to explicitly go and find and use that in your storage class

in addition to provisioner you configure parameters for storage we want to request for pv

Storage class is another abstraction level that abstracts the underlying storage provider as well as parameters for that 
storage or characteristics for that storage like what disktype

How is it used (StorageClass)?
same as pv it is claimed using pvc
in the pvc you add an additional attribute called storageClass claim that references the storage class to be used to create a pv that 
sastifies the claims of this pvc

WORKFLOW: 
when a pod claims storage via pvc => the pvc will claim storage from the storage class which will then provision or create
a pv that meets the needs of that claim using provisioner from the actual storage backend 


K8s stateful sets explained:
Its the k8s component that is used specifically for stateful applications
Stateful application:
=> databases mysql, elasticsearch, mongodb etc
=> Any application that stores data to keep track of its state

These are applications that tracks state by keeping that information in some storage

stateless applications do not keep record of previous interactions(state), in each request is handled
as compeletely new isolated interaction based entirely on the information that comes with it and sometimes
stateless applications connect to the stateful applications to foward those requests

Imagine the node-mongo application, the request that comes in dont depend on the previous information 
but entirely on the payload in the request itself
Typically such request will want to update or query the data and thats where you use the database and 
the database will update the data based on the previous state or query the data from its storage

so the database depends on the up to date data/state but:
for the node app its just the passthrough for data query/update and it just processes the code

Because of this difference they are deployed in different ways using different component in k8s and 
deployment allows you to replicate that application in the cluster



So stateful applications are deployed using statefulSet k8s component and just like deployment stateful 
sets allows you to replicate the app and run multiple replicas of it

They both manage pods based on container specification; You can configure storage in similar ways

If they both do the same thing then what is the difference and why use different one for each type of 
application


Deployment vs stateful sets:
-> replicating stateful application is more difficult
-> Has other couple of requirement that stateless application do not have
The java-mysql applicatin will be identical and interchangable, sp you can scale it using deployment pretty
easly, deployment will create the required pods in random order with random hashes at the end of the podname 
(eg. pod-name-3hj5jh53h4lk3243)
-> one service load balances to any pod and when you delete them they get deleted in a random order or when you
scale them down one random replica gets chosen to be deleted

On the other hand:
=> mysql pods can not be created/deleted at any time and in any otder
=> cant be randomly adressed

and the reason for that the pod replicas are not identical
-> Pod identity => Infact they each have their own identity on top of the common blueprint they get created
from and giving each pod its individual identity is what the statefulSet does different from deployment


It maintains sticky identity for each pod
Created from same specification but not interchangable
Each has a persistence identifier that it maintains across any re-scheduling, meaning when a pod dies and it
gets replaced by a new pod it keeps that identity

Why is this identity neccesary
this is a concept that you should understand about scaling databases in general: When you start a mysql pod 
it will be used for reading and writing data and when you add a second one it can not act the same way 
because if you allow two independent instances of mysql to change the same data you will end up with data inconsistencies

There is a mechanism that allows one pod to write or change the data which is shared reading at the, reading at the same time 
multiple instances of databases is compeletely fine

The pod that is allowed to change the data is called tha master and the other are called slaves/worker nodes
This is what differentiating one pod from the other, so not all pods are identical 

These pods do not have acess to the same physical storage even though they use the same data they are not
using the same physical storage of the data, they each have their own replicas of the storage  that each one 
of them can access for itself, that means each replica at any time must have the same data as the other one

And in order to achieve that they have to continously synchronize data, since the master is the only one allowed
to change state then the workers must knpw about change to be up to date, so that they can update their own storage and 
for the next query request

There a mechanism for cluster data synchronization, When a new pod is created it first clone the previos pod and not any
pod in the setup but always from the previous pods and once it has all the data cloned it then starts continous synchronization as well

This means you can have temporary storage in the application any not persist the data at all since the data gets replicated by the pods
so theoretically its possible but this means that all the data will be lost when all the pods dies, for instance in crashes. So its a good
practice to use persistent volumes if losing the data is unacceptable which is the case in most database applications

and with persistence storage data will survive even if the pods of all the stateful application dies, even if everything is wiped out
Because persistent volumes lifecycle isnt tied to other component's lifecycle


Each pod has its own persistent volume which is backed by a physical storage, since the pods info is stored in the database and is persistent
when a pod dies and a new one is created the new one gets re-attached to the replacement pod using the identifier that is unique, the storagge 
the state of the pod in saddition to the replicated data. Identity state is kept  and for reattachment to work it is important to use the remote
storage

Every pod has its own identifier, unlike the deployments the stateful sets gets fixed order names which is made up of the stateful name and 
the ordinal
syntax: $(statefulset name)-$(ordinal)


The first one is master and comes the slaves in the order of startup

NOTE: the next pod is only created if PREVIOUS is up and running, for instance if the first one is pending the next one cant be created at all

==>> the same order is followed in deletion but in reversr order, for instance if you scale it down from three to one the deletion will start
from the last one and it will wait until the lasr one has finished deleting before going to the next

All these mechanisms are in place in order to protect the data and state depends on

Each stateful pod gets dns endpoint from a service they are loadbalanced the same way as the service and the dns name:
syntax: $(podname).$(gorverning service domain)



These characretistics:
=> having a predictable name
=> having a fixed individual DNS name 
Means that when the pod restarts the IP adress the ip adress will change but the name and endpoint stays the same
The sticky identity makes sure that each replica pod can retain its state and its role even when it dies 
and gets recreated

Replocating stateful storage with its persistence storage requires a complex mechanism	k8s helps to set it up but youo still need to do a lot by yourself where k8s 
doesnt provide out of the box solution

For example you need to configure the cloning and data synchronization inside the stateful sets and also make the remote storage available 
as well as taking care of and managing the backups

Stateful applications are not perfect in containerized environments



K8s services explained:
Compelete overview of k8s services:
=> What is k8s service and when do we need it?

in k8s cluster each pod gets its own ip adress but the pods in k8s are ephemeral meaning they are destroyed frequently
, the new one after restart gets the new ip adress
For the service its ip adress is manual and stays even when the pod dies, so infront of every pod we set a service which
has a stable persistent ip adress 

Service also provides load balancing, the service will get the request and foward it to one of the pods
Services are a good abstraction for a good coupling for communication with in the cluster and also from the external service
like if you have browser request coming to the cluster or talking to an external database 


=> Different service types
  cluster ip service, headless service, nodePort services and Loadbalancer services

clusterIP:
default
Sidecar container collects the logs of the main container or microservice and sends them to the final 
destination in the database 
pods will also have an IP from node range, each worker node will get a range of ip adresses which are internal in the cluster
To see the ip adresses of the pods
command: kubectl get pod -o wide
This will give the extended output of the pod's information


How does the request from the browser reach the pods is through the service, clusterIP or so called an internal service

Service is just an abstraction it is just an ip adress that is accesible at and service just like the pods will be available at certain ports

We define the service in ingress by its name and the dns resolution maps its name to the ip adress

After the request reaches the service the service will know how to foward the request to one of its pods that are registered as the 
service endpoints

How do the service know which pod its managing or to foward the request to and how does it know which port to foward the request to  on that soeciefic pod
===> the service identifies its member pods via selectors attribute in the spec: selectors: you can define the key value pairs which are labels to match that 
	selector 
so in the pods we assign the labels of pods in the metadata section, this name can be arbitary 
===> and in the service yaml file we define selectors to match any pod that has all of these labels
====> You can define more than 1 label you want to match 

Synch matches all the pods replicas with those selectors and registers them as the endpoints and it should match all the selectors 
not just one

How does the service know which port to use as the pod can have more than one container?
==> This is defined in the target port attribute  and when a request comes it will pick one of those ports randomly and it will send the request it receives to that
speciefic pod on the port defined by the target port attribute in this case 3000

k8s creates the endpont object with the same name as the service and keeps track of which pods are members/endpoints of the service

Service communication port vs target port
==>> Service port is arbitary  but target port must match the port, the container is litsening to 

Take an example of the pod trying to access the database it will communicate with the database via the port that it has open(google this)

2:24:00

=> Difference between services and when to use which one


=> Kubernetes and generally containerized environments are perfectly fitting for stateless applications

that do not have any state and data dependency, scaling and replocating them is super easy


